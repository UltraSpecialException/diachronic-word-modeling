{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Diachronic Sense Modelling",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d065faccd77341a4a8b22102881ac02b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a574f725d1c444abadb2cf53216e1590",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ba866c1f71ba4301bad5402b10e73e37",
              "IPY_MODEL_ebf0ab08b7e847e790fe36ff4932ad56"
            ]
          }
        },
        "a574f725d1c444abadb2cf53216e1590": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba866c1f71ba4301bad5402b10e73e37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8963650b63704e3fb73f5a25dc73130c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cf37198c51bf4d14ab4ff5bb9fdf3ef3"
          }
        },
        "ebf0ab08b7e847e790fe36ff4932ad56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d9c378d87bab4f11addf121af36a00dd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 309kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9c2afb586dee45aaa080b7d123b8fb53"
          }
        },
        "8963650b63704e3fb73f5a25dc73130c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cf37198c51bf4d14ab4ff5bb9fdf3ef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d9c378d87bab4f11addf121af36a00dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9c2afb586dee45aaa080b7d123b8fb53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "88adb5350c2e4d3bad4212ac04b09dd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_36a9f99884fa43868ab4812dc6aba289",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_449150a19dfa4688a592e668d914316e",
              "IPY_MODEL_1ac0a48deecf41ffa859b2fc6180b89d"
            ]
          }
        },
        "36a9f99884fa43868ab4812dc6aba289": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "449150a19dfa4688a592e668d914316e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_de71c2e814f34070b308543e5d38808e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 546,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 546,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f519cd827b2c456a826312da25ca1489"
          }
        },
        "1ac0a48deecf41ffa859b2fc6180b89d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_84b7e1b952f24633a17174e2f08ef79c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 546/546 [00:25&lt;00:00, 21.0B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_940b1f9f5fb547659af2fa3b93d273e5"
          }
        },
        "de71c2e814f34070b308543e5d38808e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f519cd827b2c456a826312da25ca1489": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "84b7e1b952f24633a17174e2f08ef79c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "940b1f9f5fb547659af2fa3b93d273e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a164bc1440e84f30a76091c62e434191": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b0348ad401014bd2b9c5c9dffd2ce3ec",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_57c29084b67a4d92b8f3b59871352bd7",
              "IPY_MODEL_848234cd23ad43159896a63b390a856e"
            ]
          }
        },
        "b0348ad401014bd2b9c5c9dffd2ce3ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "57c29084b67a4d92b8f3b59871352bd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4bfde6ad260c4f799bd2540ccb896e41",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 267967963,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 267967963,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1af3fd7488154ed69f39fe1e79669b08"
          }
        },
        "848234cd23ad43159896a63b390a856e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_57cbef6783fb4932a94fe2291315f02e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 268M/268M [00:24&lt;00:00, 11.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_02970632c569460ba6658df75680f4fc"
          }
        },
        "4bfde6ad260c4f799bd2540ccb896e41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1af3fd7488154ed69f39fe1e79669b08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "57cbef6783fb4932a94fe2291315f02e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "02970632c569460ba6658df75680f4fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7W3gEig2J6D",
        "colab_type": "text"
      },
      "source": [
        "# Tracking of sense gains and losses of words over time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MujqmEVDBJ22",
        "colab_type": "text"
      },
      "source": [
        "Some setting up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GEmbGuDBL1h",
        "colab_type": "code",
        "outputId": "009927d6-9f1d-4d04-8923-03c1d1aca39f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/gdrive\")\n",
        "except ModuleNotFoundError:\n",
        "  print(\"This notebook is not currently using Google Colab.\")\n",
        "\n",
        "\n",
        "do_train_bert = False\n",
        "do_train_xlnet = False"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69Y6uc3cBq4d",
        "colab_type": "code",
        "outputId": "a83b3740-d006-47b0-cd63-18f1f2f72efc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "% cd /content/gdrive/My\\ Drive/UofT/CSC2611/Project"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/UofT/CSC2611/Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKgDUhyY2V8q",
        "colab_type": "text"
      },
      "source": [
        "We will be fine-tuning a pretrained BERT model in this investigation from \n",
        "\n",
        "---\n",
        "\n",
        "Hugging Face's repository, so we will need to install it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laizc84o2TEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  import transformers\n",
        "except ModuleNotFoundError: \n",
        "  print(\"Module transformers not found, trying to install from cache...\")\n",
        "  try:\n",
        "    % pip install --upgrade --force-reinstall `cat/content/gdrive/My\\ Drive/colab_installed.txt`\n",
        "    import transformers\n",
        "  except ModuleNotFoundError:\n",
        "    print(\"Cache not found, installing the package...\")\n",
        "    % pip install transformers\n",
        "    % pip freeze --local > /content/gdrive/My\\ Drive/colab_installed.txt\n",
        "    import transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSc0eRQaA_VY",
        "colab_type": "text"
      },
      "source": [
        "Some packages that need to be installed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJEIQWEeDQvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pylab\n",
        "from torch.utils.data import TensorDataset, DataLoader, \\\n",
        "    RandomSampler, SequentialSampler\n",
        "import argparse\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tqdm import tqdm\n",
        "from transformers import DistilBertForSequenceClassification, \\\n",
        "    get_linear_schedule_with_warmup, AdamW, XLNetForSequenceClassification, \\\n",
        "    DistilBertTokenizer\n",
        "from collections import defaultdict\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "from scipy.stats import spearmanr, pearsonr, rankdata\n",
        "import warnings\n",
        "import contextlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYjRnXrjY2bV",
        "colab_type": "text"
      },
      "source": [
        "Functions for preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0vjwWfiFNt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_inputs(tokenizer, sentences, add_special_tokens=True):\n",
        "    \"\"\"\n",
        "    Use the tokenizer given to tokenize the sentences into their IDs.\n",
        "    \"\"\"\n",
        "    tokenized_sentences = []\n",
        "    for sentence in sentences:\n",
        "        tokenized_sentence = tokenizer.encode(\n",
        "            sentence, \n",
        "            add_special_tokens=add_special_tokens\n",
        "            )\n",
        "        tokenized_sentences.append(tokenized_sentence)\n",
        "\n",
        "    return tokenized_sentences\n",
        "\n",
        "\n",
        "def pad_inputs(tokenized_sentences, padding_token=0):\n",
        "    \"\"\"\n",
        "    Return the padded sentences where each sentence is padded with 0's so that\n",
        "    all sentences have the length of the longest sentence.\n",
        "    \"\"\"\n",
        "    max_len = max([len(sentence) for sentence in tokenized_sentences])\n",
        "\n",
        "    return pad_sequences(tokenized_sentences, maxlen=max_len, dtype=\"long\",\n",
        "                         value=padding_token, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "\n",
        "def get_padding_mask(padded_sentences):\n",
        "    \"\"\"\n",
        "    Return a list of masks, one for each tokenized sentence, where at each\n",
        "    position of the sentence, if the token has a non-zero value, then the token\n",
        "    has meaning and thus is not a padding token and will have value 1 in the\n",
        "    corresponding position in the mask, otherwise, the token is a padding token\n",
        "    and will have value 0 in the corresponding position in the mask.\n",
        "    \"\"\"\n",
        "    return padded_sentences > 0\n",
        "\n",
        "\n",
        "def get_train_val_loader(inputs, masks, labels, batch_size, train_split=0.8):\n",
        "    \"\"\"\n",
        "    Return the train and validation data loader.\n",
        "    \"\"\"\n",
        "    assert 0 < train_split < 1, \\\n",
        "        \"train_split needs to be a fraction between 0 and 1 exclusive\"\n",
        "    num_train = int(np.ceil(inputs.size(0) * train_split))\n",
        "    num_val = int(inputs.size(0) - num_train)\n",
        "\n",
        "    assert num_train and num_val, \\\n",
        "        f\"the train_split given ({train_split}) resultted in either the \" \\\n",
        "        f\"number of training or validation examples being 0, which is \" \\\n",
        "        f\"invalid\"\n",
        "\n",
        "    permuted_indices = torch.randperm(inputs.size(0))\n",
        "    train_indices = permuted_indices[: num_train]\n",
        "    val_indices = permuted_indices[num_train: ]\n",
        "\n",
        "    train_inputs, train_masks, train_labels = \\\n",
        "        inputs[train_indices], masks[train_indices], labels[train_indices]\n",
        "\n",
        "    val_inputs, val_masks, val_labels = inputs[val_indices], \\\n",
        "                                        masks[val_indices], labels[val_indices]\n",
        "\n",
        "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, batch_size, sampler=train_sampler)\n",
        "\n",
        "    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "    val_dataloader = DataLoader(val_data, batch_size, sampler=val_sampler)\n",
        "\n",
        "    return train_dataloader, val_dataloader\n",
        "\n",
        "def check_path(path):\n",
        "    \"\"\"\n",
        "    Check whether or not the path given is a valid path in this OS.\n",
        "    \"\"\"\n",
        "    full_path = path\n",
        "    if full_path[-1] == \"/\":\n",
        "        full_path = full_path[:-1]\n",
        "\n",
        "    parent_end = full_path.rfind(\"/\")\n",
        "    if parent_end == -1:\n",
        "        parent_end = 0\n",
        "    parent = os.path.join(\n",
        "        os.path.abspath(\".\"),\n",
        "        full_path[:parent_end]\n",
        "    )\n",
        "    if not os.path.isdir(parent):\n",
        "        raise FileNotFoundError(f\"parent path {parent} not found\")\n",
        "\n",
        "\n",
        "def set_seed(seed, modules=()):\n",
        "  \"\"\"\n",
        "  Seed the random modules.\n",
        "  \"\"\"\n",
        "  for mod in modules:\n",
        "    mod(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccEOb3pQY9kj",
        "colab_type": "text"
      },
      "source": [
        "The fine-tuning architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt3C6BwTSlvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "name_to_activation = {\n",
        "    \"relu\": nn.ReLU,\n",
        "    \"gelu\": nn.GELU\n",
        "}\n",
        "\n",
        "\n",
        "class DistilBertWordSenseDisambiguation(DistilBertForSequenceClassification):\n",
        "    \"\"\"\n",
        "    Apply BERT for Word Sense Disambiguation task.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initialize an instance of WSD BERT.\n",
        "        \"\"\"\n",
        "        super(DistilBertWordSenseDisambiguation, self).__init__(config)\n",
        "        self.config_ = config\n",
        "\n",
        "    def reset_classifier(self, num_layers, activation):\n",
        "        \"\"\"\n",
        "        Reset the classifier to default initialization.\n",
        "        \"\"\"\n",
        "        config = self.config_\n",
        "        blocks = []\n",
        "        for _ in range(num_layers):\n",
        "            layer = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "            nn.init.xavier_uniform_(layer.weight)\n",
        "            nn.init.constant_(layer.bias, 0)\n",
        "            blocks.append(layer)\n",
        "            blocks.append(name_to_activation[activation]())\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            *blocks,\n",
        "            nn.Linear(config.hidden_size, self.config.num_labels)\n",
        "        )\n",
        "\n",
        "\n",
        "class XLNetWordSenseDisambiguation(XLNetForSequenceClassification):\n",
        "    \"\"\"\n",
        "    Apply XLNet for Word Sense Disambiguation task.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initialize an instance of WSD BERT.\n",
        "        \"\"\"\n",
        "        super(XLNetWordSenseDisambiguation, self).__init__(config)\n",
        "        self.config_ = config\n",
        "\n",
        "    def reset_classifier(self, num_layers, activation):\n",
        "        \"\"\"\n",
        "        Reset the classifier to default initialization.\n",
        "        \"\"\"\n",
        "        config = self.config_\n",
        "        blocks = []\n",
        "        for _ in range(num_layers):\n",
        "            layer = nn.Linear(config.d_model, config.d_model)\n",
        "            nn.init.xavier_uniform_(layer.weight)\n",
        "            nn.init.constant_(layer.bias, 0)\n",
        "            blocks.append(layer)\n",
        "            blocks.append(name_to_activation[activation]())\n",
        "\n",
        "        self.logits_proj = nn.Sequential(\n",
        "            *blocks,\n",
        "            nn.Linear(config.d_model, self.config.num_labels)\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8rFG1IkiHlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# our training configurations\n",
        "seed = 42\n",
        "num_layers = 2\n",
        "activation = \"gelu\"\n",
        "epochs = 1\n",
        "batch_size = 8\n",
        "data_path = \"data/semcor_training_data.txt\"\n",
        "bert_save_outputs_weights = \"bert_WSD.tar\"\n",
        "xlnet_save_outputs_weights = \"xlnet_WSD.tar\"\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PdhQj3mj1-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data that is processed previously and now we just load it instead of \n",
        "# tokenizing the entire dataset again\n",
        "padded_data_input = \"finetune/tokenized_semcor_data.tar\"\n",
        "\n",
        "assert ((do_train_bert and \"xlnet\" not in padded_data_input) \n",
        "or (do_train_xlnet and \"xlnet\" in padded_data_input) \n",
        "or not (do_train_bert or do_train_xlnet))\n",
        "\n",
        "if do_train_bert or do_train_xlnet:\n",
        "  print(\"Loading data...\")\n",
        "  padded_data = torch.load(padded_data_input, map_location=\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7C3b8_omBq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if do_train_bert or do_train_xlnet:\n",
        "  plain_data = open(data_path)\n",
        "  data = plain_data.readlines()\n",
        "  plain_data.close()\n",
        "  labels = []\n",
        "  print(\"Getting labels...\")\n",
        "  for line in data:\n",
        "      _, label = line.strip().split(\"@\")\n",
        "      labels.append(int(label))\n",
        "\n",
        "  del data\n",
        "\n",
        "  labels = torch.tensor(labels)\n",
        "  masks = get_padding_mask(padded_data)\n",
        "  if do_train_xlnet:\n",
        "    masks = masks.int()  \n",
        "\n",
        "  print(\"Getting data loaders...\")\n",
        "  # from our data, labels, and masks, we create iterators for our\n",
        "  # training loop\n",
        "  train_dataloader, val_dataloader = get_train_val_loader(\n",
        "      padded_data,\n",
        "      masks,\n",
        "      labels,\n",
        "      batch_size\n",
        "  )\n",
        "\n",
        "  # delete these unused variables to save RAM\n",
        "  del padded_data\n",
        "  del masks\n",
        "  del labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSWqujvIZSXw",
        "colab_type": "text"
      },
      "source": [
        "The training and validation loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX_fVzjIntuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses = []\n",
        "def train(model, optimizer, scheduler, device, name=\"bert\", max_iter=None):\n",
        "  \"\"\"\n",
        "  Train the model with the arguments given to the optimizer.\n",
        "  The scheduler varies the learning rate. If max_iter is not None, stop the\n",
        "  training when we have trained max_iter iterations.\n",
        "  \"\"\"\n",
        "  step = 1\n",
        "  total_iters = len(train_dataloader) * epochs\n",
        "  total_loss = 0\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for inputs, masks, labels in train_dataloader:\n",
        "      model.zero_grad()\n",
        "      inputs = inputs.to(device)\n",
        "      masks = masks.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = model(\n",
        "          inputs,\n",
        "          attention_mask=masks,\n",
        "          labels=labels\n",
        "      )\n",
        "\n",
        "      loss = outputs[0]\n",
        "      total_loss += loss.item()\n",
        "      train_losses.append(loss)\n",
        "      losses.append(loss)\n",
        "      if not (step % 100):\n",
        "        print(f\"At iteration {step}/{total_iters}; Avg Loss: {total_loss/100}\")\n",
        "        total_loss = 0\n",
        "        if not (step % 1000):\n",
        "          print(f\"Saving checkpoint at step {step}...\")\n",
        "          torch.save(model.state_dict(), f\"checkpoints/{name}_checkpoint{step}.tar\")\n",
        "          print(\"Checkpoint saved...\")\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "      if max_iter is not None and step >= max_iter:\n",
        "        break\n",
        "        \n",
        "      step += 1\n",
        "\n",
        "      del loss\n",
        "      del inputs\n",
        "      del masks\n",
        "      del labels\n",
        "      del outputs\n",
        "      torch.cuda.empty_cache()\n",
        "  return train_losses \n",
        "\n",
        "\n",
        "def validate(model, device, max_batches=None):\n",
        "  \"\"\"\n",
        "  Validate the model by running a holdout dataset through the model and\n",
        "  computing the accuracy. If max_batches is not None, we only validate using\n",
        "  this number of batches.\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  step = 1\n",
        "  for inputs, masks, labels in val_dataloader:\n",
        "    inputs = inputs.to(device)\n",
        "    masks = masks.to(device)\n",
        "    labels = labels.to(device)\n",
        "    with torch.no_grad():\n",
        "      outputs = model(\n",
        "          inputs,\n",
        "          attention_mask=masks,\n",
        "          labels=labels\n",
        "      )\n",
        "\n",
        "    loss, logits = outputs[:2]\n",
        "\n",
        "    preds = logits.argmax(dim=1).flatten()\n",
        "    correct += (preds == labels).sum()\n",
        "    total += preds.size(0)\n",
        "\n",
        "    del loss\n",
        "    del inputs\n",
        "    del masks\n",
        "    del labels\n",
        "    del outputs\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    if max_batches is not None and step >= max_batches:\n",
        "        break\n",
        "    \n",
        "    step += 1\n",
        "  print(f\"Validation accuracy: {correct.item()/total}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doj9fQ_zppZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_functions = [np.random.seed, torch.manual_seed]\n",
        "if torch.cuda.is_available():\n",
        "  seed_functions.append(torch.cuda.manual_seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC5NpaH6h8yC",
        "colab_type": "text"
      },
      "source": [
        "Finetuning BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2Jm0vD3h-Kl",
        "colab_type": "code",
        "outputId": "8402242c-55d2-44ec-8a05-0f298af346d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181,
          "referenced_widgets": [
            "d065faccd77341a4a8b22102881ac02b",
            "a574f725d1c444abadb2cf53216e1590",
            "ba866c1f71ba4301bad5402b10e73e37",
            "ebf0ab08b7e847e790fe36ff4932ad56",
            "8963650b63704e3fb73f5a25dc73130c",
            "cf37198c51bf4d14ab4ff5bb9fdf3ef3",
            "d9c378d87bab4f11addf121af36a00dd",
            "9c2afb586dee45aaa080b7d123b8fb53",
            "88adb5350c2e4d3bad4212ac04b09dd2",
            "36a9f99884fa43868ab4812dc6aba289",
            "449150a19dfa4688a592e668d914316e",
            "1ac0a48deecf41ffa859b2fc6180b89d",
            "de71c2e814f34070b308543e5d38808e",
            "f519cd827b2c456a826312da25ca1489",
            "84b7e1b952f24633a17174e2f08ef79c",
            "940b1f9f5fb547659af2fa3b93d273e5",
            "a164bc1440e84f30a76091c62e434191",
            "b0348ad401014bd2b9c5c9dffd2ce3ec",
            "57c29084b67a4d92b8f3b59871352bd7",
            "848234cd23ad43159896a63b390a856e",
            "4bfde6ad260c4f799bd2540ccb896e41",
            "1af3fd7488154ed69f39fe1e79669b08",
            "57cbef6783fb4932a94fe2291315f02e",
            "02970632c569460ba6658df75680f4fc"
          ]
        }
      },
      "source": [
        "pretrained_model = \"distilbert-base-uncased\"\n",
        "\n",
        "print(f\"Using pretrained model: {pretrained_model}\")\n",
        "bert_tokenizer = DistilBertTokenizer.from_pretrained(pretrained_model)\n",
        "model = DistilBertWordSenseDisambiguation.from_pretrained(\n",
        "    pretrained_model,\n",
        "    num_labels=2,\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False\n",
        ")\n",
        "model.reset_classifier(num_layers, activation)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using pretrained model: distilbert-base-uncased\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d065faccd77341a4a8b22102881ac02b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_widâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88adb5350c2e4d3bad4212ac04b09dd2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=546, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a164bc1440e84f30a76091c62e434191",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=267967963, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGuWSpl3iE7o",
        "colab_type": "code",
        "outputId": "2340c307-87d0-4f21-eba5-9582abe916c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from_checkpoint = True\n",
        "checkpoint = \"checkpoints/bert_WSD.tar\"\n",
        "max_iter = None\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "bert_model = model\n",
        "if from_checkpoint:\n",
        "  print(\"Loading checkpoint...\")\n",
        "  bert_model.load_state_dict(torch.load(checkpoint, map_location=\"cpu\"))\n",
        "bert_model = bert_model.to(device)\n",
        "do_train = False\n",
        "if do_train:\n",
        "  set_seed(seed, seed_functions)\n",
        "  total_iterations = len(train_dataloader) * epochs\n",
        "  parameters = []\n",
        "  for name, param in model.named_parameters():\n",
        "    if name == \"distilbert.embeddings.word_embeddings.weight\" or \\\n",
        "      name.startswith(\"classifier\"):\n",
        "        parameters.append(param)\n",
        "    else:\n",
        "      param.requires_grad_(False)\n",
        "  parameters = nn.ParameterList(parameters)\n",
        "  bert_optimizer = AdamW(parameters, lr=2e-5)\n",
        "  bert_scheduler = get_linear_schedule_with_warmup(\n",
        "      bert_optimizer, 0, total_iterations)\n",
        "\n",
        "  train_loss = train(bert_model, bert_optimizer, bert_scheduler, device)\n",
        "  torch.save(bert_model.state_dict(), bert_save_outputs_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SslMUd1yr5LT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_bert = True\n",
        "if test_bert:\n",
        "  validate(bert_model, device, 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZkg1GVqaAq4",
        "colab_type": "text"
      },
      "source": [
        "Fine-tuning XLNet\n",
        "\n",
        "Due to limited memory on the GPU, this isn't feasible on Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SF0W-W1oqRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pretrained_model = \"xlnet-base-cased\"\n",
        "# print(f\"Using pretrained model: {pretrained_model}\")\n",
        "# model = XLNetWordSenseDisambiguation.from_pretrained(\n",
        "#     pretrained_model,\n",
        "#     num_labels=2,\n",
        "#     output_attentions=False,\n",
        "#     output_hidden_states=False\n",
        "# )\n",
        "# model.reset_classifier(num_layers, activation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q85mj5UdpbnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# xlnet_model = model\n",
        "# checkpoint_path = \"checkpoints/xlnet_checkpoint6000.tar\"\n",
        "# from_checkpoint = True\n",
        "# if from_checkpoint:\n",
        "#   xlnet_model.load_state_dict(torch.load(checkpoint_path, map_location=\"cpu\"))\n",
        "\n",
        "# xlnet_model = xlnet_model.to(device)\n",
        "# max_iter = 205741 - 6000\n",
        "\n",
        "# if do_train_xlnet:\n",
        "#   set_seed(seed, seed_functions)\n",
        "#   torch.cuda.empty_cache()\n",
        "#   total_iterations = len(train_dataloader) * epochs\n",
        "#   xlnet_parameters = []\n",
        "#   for name, param in model.named_parameters():\n",
        "#     if name == \"transformer.word_embedding.weight\" or \\\n",
        "#       name.startswith(\"logits_proj\"):\n",
        "#         xlnet_parameters.append(param)\n",
        "#     else:\n",
        "#       param.requires_grad_(False)\n",
        "#   xlnet_parameters = nn.ParameterList(xlnet_parameters)\n",
        "#   xlnet_optimizer = AdamW(xlnet_parameters, lr=2e-5)\n",
        "#   xlnet_scheduler = get_linear_schedule_with_warmup(\n",
        "#       xlnet_optimizer, 0, total_iterations)\n",
        "\n",
        "#   train_loss = train(xlnet_model, xlnet_optimizer, xlnet_scheduler, device, \"xlnet\", max_iter)\n",
        "#   torch.save(xlnet_model.state_dict(), xlnet_save_outputs_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4kr9QdKEngT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_bert_sense_embedding(sentences, target, sense, model, tokenizer, device, return_sum=False):\n",
        "  \"\"\"\n",
        "  Return the \"sense embedding\" by feeding sentence with the target word\n",
        "  with its corresponding sense and getting the associated word emebdding.\n",
        "  \"\"\"\n",
        "  tokenized_sentences = tokenize_inputs(\n",
        "      tokenizer, sentences, add_special_tokens=True)\n",
        "  tokenized_target = tokenizer.encode(target, add_special_tokens=False)[0]\n",
        "  assert all(tokenized_target in tokenized_sentence \n",
        "              for tokenized_sentence in tokenized_sentences)\n",
        "  target_indices = [tokenized_sentence.index(tokenized_target) \n",
        "                    for tokenized_sentence in tokenized_sentences]\n",
        "\n",
        "  target_indices = torch.tensor(target_indices)\n",
        "\n",
        "  padded_sentences = pad_inputs(tokenized_sentences, tokenizer.pad_token_id)\n",
        "  padded_sentences = torch.tensor(padded_sentences)\n",
        "  masks = get_padding_mask(padded_sentences)\n",
        "  padded_sentences = padded_sentences.to(device)\n",
        "  masks = masks.to(device)\n",
        "\n",
        "  bert = model.distilbert\n",
        "  hidden_states = bert(padded_sentences, attention_mask=masks)[0]\n",
        "\n",
        "  shape = hidden_states.size()\n",
        "  num_sentences = len(sentences)\n",
        "  hidden_states = hidden_states.view(-1, shape[-1])\n",
        "  target_indices += torch.arange(0, shape[0] * shape[1], shape[1])\n",
        "  if not return_sum:\n",
        "    sense_embedding = hidden_states[target_indices].mean(dim=0)\n",
        "  else:\n",
        "    sense_embedding = hidden_states[target_indices].sum(dim=0)\n",
        "\n",
        "  sense_embedding = sense_embedding.to(torch.device(\"cpu\"))\n",
        "\n",
        "  del hidden_states\n",
        "  return sense_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w52d3t0dtn6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_words = [\n",
        "                \"face\", \"part\", \"head\", \"record\", \"word\", \"edge\", \"land\", \n",
        "                \"circle\", \"relationship\", \"rag\", \"fiction\", \"ball\", \"plane\", \n",
        "                \"risk\", \"gas\", \"ounce\", \"bag\", \"prop\", \"bit\", \"tree\", \"twist\", \n",
        "                \"attack\", \"savage\", \"tip\", \"pin\", \"player\", \"contemplation\", \n",
        "                \"lane\", \"stroke\", \"thump\", \"stab\", \"chairman\"\n",
        "                ]\n",
        "\n",
        "sentences_and_word_senses = json.load(open(\n",
        "    \"words_with_sentences_separated_by_senses_pos_big.json\"\n",
        "))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncWJ3LYJzrul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine_similarity(possible, true):\n",
        "  \"\"\"\n",
        "  Measure the cosine distance between each vector in possible\n",
        "  with the vector true.\n",
        "\n",
        "  Possible should have shape (n, hidden_dim) and true should have shape\n",
        "  (hidden_dim,), where n is the number of vectors. \n",
        "  \"\"\"\n",
        "  assert possible.dim() == 2 and true.dim() == 1\n",
        "  dot_products = torch.einsum(\"ij,j->i\", possible, true)\n",
        "  norms_possible = torch.norm(possible, dim=1)\n",
        "  norm_true = torch.norm(true)\n",
        "  \n",
        "  return dot_products / (norms_possible * norm_true)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2Z7xirw8sYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_sense_freq(corpus, sense_embeddings, device, has_pos=True):\n",
        "  \"\"\"\n",
        "  For the corpus <corpus>, count the number of times each sense of a word is\n",
        "  used. A sense of a target word in a sentence is the sense that has the\n",
        "  highest cosine similarity between the sense embedding and the target word's\n",
        "  word embedding.\n",
        "  \"\"\"\n",
        "  # the dictionary of word: sense: count which we will populate and return\n",
        "  word_sense_to_freq = {word: {} for word in target_words}\n",
        "\n",
        "  for word in corpus:\n",
        "    # the corpus is a dictionary of words to the associated sentences where\n",
        "    # the word has a labelled sense, we only want to process a subset of\n",
        "    # target words\n",
        "    if word not in target_words:\n",
        "      continue\n",
        "    print(\"Getting embeddings for word\", word)\n",
        "\n",
        "    # we want to get the sense embeddings of all the senses of this word\n",
        "    embeddings = {}\n",
        "\n",
        "    # the target words in the testing set has an associated POS tag, so when\n",
        "    # construct sense embeddings, we eliminate senses that are not of the \n",
        "    # correct POS, some words only have senses of a particular POS in the corpus\n",
        "    # and by eliminating it, we might not have any sense left for the word\n",
        "    # we omit/skip these words\n",
        "    if not word_sense_to_id[word]:\n",
        "      print(f\"Skipping word {word} because it has no available senses\")\n",
        "      continue\n",
        "\n",
        "    # here, we get the sense embeddings for each of the sense by loading\n",
        "    # the saved tensor to memory\n",
        "    # sense_embeddings is a dictionary of \n",
        "    # word: sense: <path to saved tensor of sense embedding>\n",
        "    for sense in word_sense_to_id[word]:\n",
        "      sense_id = word_sense_to_id[word][sense]\n",
        "      word_sense_to_freq[word][sense_id] = 0\n",
        "      embeddings[sense_id] = torch.load(sense_embeddings[word][sense])\n",
        "\n",
        "    # now we do the counting for the senses of the word \n",
        "    for sentence in corpus[word]:\n",
        "      tokenized = bert_tokenizer.encode(sentence)\n",
        "      word_id = bert_tokenizer.encode(word, add_special_tokens=False)[0]\n",
        "\n",
        "      # we get the index of the target word in the sentence\n",
        "      index = tokenized.index(word_id)\n",
        "      tokenized = torch.tensor(tokenized).to(device).view(1, -1)\n",
        "\n",
        "      # the word embedding of the target word in this sentence is obtained by\n",
        "      # accessing the final hidden layer after passing the sentence through\n",
        "      # the model\n",
        "      word_embedding = bert_model.distilbert(tokenized)[0][0][index]\n",
        "\n",
        "      # we compute the cosine similarity scores between all the senses available\n",
        "      # and the word embedding of the word\n",
        "      possible = torch.cat([*embeddings.values()]).view(len(embeddings), -1)\n",
        "      possible = possible.to(device)\n",
        "      similarity = cosine_similarity(possible, word_embedding)\n",
        "\n",
        "      # the sense of the word is the sense with the highest similarity score\n",
        "      correct_sense_id = similarity.argmax().item()\n",
        "      word_sense_to_freq[word][correct_sense_id] += 1\n",
        "  \n",
        "    del embeddings\n",
        "  return word_sense_to_freq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOn8bc8f4lJb",
        "colab_type": "text"
      },
      "source": [
        "Sense frequencies analysis\n",
        "\n",
        "Note: To avoid having to recompute these values everytime the notebook is restarted, the computed values are saved and loaded. Without the correct files, the files cannot be properly loaded and the recomputations would need to be done. Without a GPU, this part might take quite a while."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OAY5rFv_Xh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_frequencies = False\n",
        "\n",
        "# Count the frequencies and save the results\n",
        "if get_frequencies:\n",
        "  # % rm sense_embeddings/*\n",
        "\n",
        "  word_sense_to_id = {word: {} for word in target_words}\n",
        "  for word in sentences_and_word_senses:\n",
        "    for i, sense in enumerate(sentences_and_word_senses[word]):\n",
        "      word_sense_to_id[word][sense] = i\n",
        "\n",
        "  sense_embeddings = {word: {} for word in sentences_and_word_senses}\n",
        "  for word in sentences_and_word_senses:\n",
        "    print(f\"Processing word {word}\")\n",
        "    for sense in sentences_and_word_senses[word]:\n",
        "      sentences = sentences_and_word_senses[word][sense]\n",
        "      num_sentences = len(sentences)\n",
        "      embedding = torch.zeros(768)\n",
        "      for i in range(len(sentences)):\n",
        "        batch = [sentences[i]]\n",
        "        batch_embedding = get_bert_sense_embedding(\n",
        "            batch, word, sense, bert_model, bert_tokenizer, device, True)\n",
        "        embedding += batch_embedding\n",
        "        del batch_embedding\n",
        "        torch.cuda.empty_cache()\n",
        "      embedding /= num_sentences\n",
        "      sense_id = word_sense_to_id[word][sense]\n",
        "      path = f\"sense_embeddings/{word}_{sense_id}_embedding_pos.tar\"\n",
        "      torch.save(embedding, path)\n",
        "      sense_embeddings[word][sense] = path\n",
        "      del embedding\n",
        "\n",
        "  with open(\"sense_embeddings.json\", \"w+\") as file:\n",
        "    json.dump(sense_embeddings, file)\n",
        "    file.close()\n",
        "\n",
        "  word_to_sentences_corpus1 = json.load(open(\"target_sent_corpus1.json\"))\n",
        "  word_to_sentences_corpus2 = json.load(open(\"target_sent_corpus2.json\"))\n",
        "\n",
        "  print(\"Analyzing corpus 1...\")\n",
        "  word_sense_to_freq_corpus1 = count_sense_freq(\n",
        "      word_to_sentences_corpus1, sense_embeddings, device)\n",
        "  print(\"Analyzing corpus 2...\")\n",
        "  word_sense_to_freq_corpus2 = count_sense_freq(\n",
        "      word_to_sentences_corpus2, sense_embeddings, device)\n",
        "  \n",
        "  print(word_sense_to_freq_corpus1)\n",
        "  with open(\"word_sense_to_id_pos.json\", \"w+\") as file:\n",
        "    json.dump(word_sense_to_id, file)\n",
        "    file.close()\n",
        "\n",
        "  with open(\"word_sense_freq_corpus1_pos.json\", \"w+\") as file:\n",
        "    json.dump(word_sense_to_freq_corpus1, file)\n",
        "    file.close()\n",
        "\n",
        "  with open(\"word_sense_freq_corpus2_pos.json\", \"w+\") as file:\n",
        "    json.dump(word_sense_to_freq_corpus2, file)\n",
        "    file.close()\n",
        "else:    # Load the results instead of recomputing\n",
        "  with open(\"word_sense_to_id_pos.json\") as file:\n",
        "    word_sense_to_id = json.load(file)\n",
        "\n",
        "  with open(\"word_sense_freq_corpus1_pos.json\") as file:\n",
        "    word_sense_to_freq_corpus1 = json.load(file)\n",
        "\n",
        "  with open(\"word_sense_freq_corpus2_pos.json\") as file:\n",
        "    word_sense_to_freq_corpus2 = json.load(file)\n",
        "\n",
        "  with open(\"sense_embeddings.json\") as file:\n",
        "    sense_embeddings = json.load(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uYKTaw4JV9D",
        "colab_type": "code",
        "outputId": "f44967d5-c2b0-4c95-bfd0-3e2c463e08cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_sense_to_freq_corpus1 == word_sense_to_freq_corpus2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZELYDagWDVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_id_to_sense = {word: {word_sense_to_id[word][k]: k \n",
        "                           for k in word_sense_to_id[word]} \n",
        "                    for word in word_sense_to_id}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAfkYjiMSFMx",
        "colab_type": "code",
        "outputId": "bbe43b3c-24f8-490f-c6de-759d7773b988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_id_to_sense[\"ball\"][1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'round object that is hit or thrown or kicked in games'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBETdXlJWWLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sense_of_word(sentence, word, sense_embeddings, device):\n",
        "  \"\"\"\n",
        "  Get the sense of the word <word> in <sentence>.\n",
        "\n",
        "  Sense embeddings should contain the embeddings for all possible\n",
        "  senses of the word <word>. \n",
        "  \"\"\"\n",
        "  # Here, we perform the same procedure as when we compute the \n",
        "  # frequencies of senses.\n",
        "\n",
        "  # At the end, we get the sense embededing with the highest similarity\n",
        "  # then we use the dictionary of sense index to sense description\n",
        "  # to return the sense of the word\n",
        "  \n",
        "  embeddings = {}\n",
        "  for sense in word_sense_to_id[word]:\n",
        "    sense_id = word_sense_to_id[word][sense]\n",
        "    embeddings[sense_id] = torch.load(sense_embeddings[word][sense])\n",
        "  tokenized = bert_tokenizer.encode(sentence)\n",
        "  word_id = bert_tokenizer.encode(word, add_special_tokens=False)[0]\n",
        "  index = tokenized.index(word_id)\n",
        "  tokenized = torch.tensor(tokenized).to(device).view(1, -1)\n",
        "  word_embedding = bert_model.distilbert(tokenized)[0][0][index]\n",
        "  possible = torch.cat([*embeddings.values()]).view(len(embeddings), -1)\n",
        "  possible = possible.to(device)\n",
        "  similarity = cosine_similarity(possible, word_embedding)\n",
        "  correct_sense_id = similarity.argmax().item()\n",
        "  return word_id_to_sense[word][correct_sense_id]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7skrslxtORho",
        "colab_type": "code",
        "outputId": "98ba076a-2ba1-476a-f590-b8854d11a777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "get_sense_of_word(\"Harry went to the Yule Ball in his fourth year.\", \"ball\", \n",
        "                  sense_embeddings, device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the people assembled at a lavish formal dance'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEvnq32RjQLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_sense_use_ratio(sense_frequencies):\n",
        "  \"\"\"\n",
        "  Compute the frequency of all senses of all words by dividing the number of\n",
        "  times a sense of a word is used by the number of times that word is used.\n",
        "  \"\"\"\n",
        "  sense_use_ratio = {}\n",
        "  for word in sense_frequencies:\n",
        "    sense_use_ratio[word] = {}\n",
        "    sense_to_freq = sense_frequencies[word]\n",
        "    total = sum(list(sense_to_freq.values()))\n",
        "    for sense in sense_to_freq:\n",
        "      sense_use_ratio[word][sense] = sense_frequencies[word][sense] / total\n",
        "\n",
        "  return sense_use_ratio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3x5zWd-nV4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sense_use_ratio_corpus1 = compute_sense_use_ratio(word_sense_to_freq_corpus1)\n",
        "sense_use_ratio_corpus2 = compute_sense_use_ratio(word_sense_to_freq_corpus2)\n",
        "sense_use_ratio_compare = {\n",
        "    word: {\n",
        "        sense: (\n",
        "            sense_use_ratio_corpus1[word][sense], \n",
        "            sense_use_ratio_corpus2[word][sense]\n",
        "            )\n",
        "        for sense in sense_use_ratio_corpus1[word]\n",
        "        } for word in sense_use_ratio_corpus1\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nDO4AtLndDs",
        "colab_type": "code",
        "outputId": "5789b668-5c96-4181-b16e-b2c7814d3da0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "for sense in sense_use_ratio_compare[\"attack\"]:\n",
        "  print(sense, word_id_to_sense[\"attack\"][int(sense)])\n",
        "sense_use_ratio_compare[\"attack\"]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 the act of attacking\n",
            "1 (military) an offensive against an enemy (using weapons)\n",
            "2 ideas or actions intended to deal with a problem or situation\n",
            "3 intense adverse criticism\n",
            "4 a decisive manner of beginning a musical tone or phrase\n",
            "5 an offensive move in a sport or game\n",
            "6 a sudden occurrence of an uncontrollable condition\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0': (0.0022026431718061676, 0.006002400960384154),\n",
              " '1': (0.18061674008810572, 0.42016806722689076),\n",
              " '2': (0.022026431718061675, 0.006002400960384154),\n",
              " '3': (0.6475770925110133, 0.3169267707082833),\n",
              " '4': (0.0022026431718061676, 0.0012004801920768306),\n",
              " '5': (0.08590308370044053, 0.07563025210084033),\n",
              " '6': (0.05947136563876652, 0.17406962785114047)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPBLtMYtnhRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"sense_use_ratio_compare.json\", \"w+\") as file:\n",
        "  json.dump(sense_use_ratio_compare, file)\n",
        "  file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8jyLbsAE4vF",
        "colab_type": "code",
        "outputId": "ec4162c9-d365-44e9-b588-3c2ff5755477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "true_data = open(\"english.txt\").readlines()[:]\n",
        "true_data = [line.split(\"\\t\") for line in true_data]\n",
        "word_to_label = {}\n",
        "for word, label in true_data:\n",
        "  stripped_word = word[:-3]\n",
        "  if stripped_word in sense_use_ratio_compare and sense_use_ratio_compare[stripped_word]:\n",
        "    word_to_label[stripped_word] = int(label.strip())\n",
        "\n",
        "word_to_label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attack': 1,\n",
              " 'bag': 0,\n",
              " 'ball': 0,\n",
              " 'bit': 1,\n",
              " 'chairman': 0,\n",
              " 'circle': 1,\n",
              " 'contemplation': 0,\n",
              " 'edge': 1,\n",
              " 'face': 0,\n",
              " 'fiction': 0,\n",
              " 'gas': 0,\n",
              " 'head': 1,\n",
              " 'land': 1,\n",
              " 'lane': 0,\n",
              " 'ounce': 0,\n",
              " 'part': 0,\n",
              " 'pin': 0,\n",
              " 'plane': 1,\n",
              " 'player': 1,\n",
              " 'prop': 1,\n",
              " 'rag': 1,\n",
              " 'record': 1,\n",
              " 'relationship': 0,\n",
              " 'risk': 0,\n",
              " 'stroke': 0,\n",
              " 'tip': 1,\n",
              " 'tree': 0,\n",
              " 'twist': 0,\n",
              " 'word': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0Cy3J_RE9q9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_accuracy(predicted, actual):\n",
        "  correct = 0\n",
        "  total = len(gain_loss)\n",
        "  for word in predicted:\n",
        "    if predicted[word] == actual[word]:\n",
        "      correct += 1\n",
        "\n",
        "  return correct / total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuPtAYr57m8b",
        "colab_type": "code",
        "outputId": "7dd8fc30-dd4d-4977-a807-55b8cbd2c1f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "gain_loss = {}\n",
        "threshold = 0.02\n",
        "for word in sense_use_ratio_compare:\n",
        "  if sense_use_ratio_compare[word]:\n",
        "    gain_loss[word] = {}\n",
        "    for sense in sense_use_ratio_compare[word]:\n",
        "      ratios = sense_use_ratio_compare[word][sense]\n",
        "      if ratios[0] < threshold and ratios[1] >= threshold:\n",
        "        gain_loss[word][sense] = \"+\"\n",
        "      elif ratios[0] >= threshold and ratios[1] < threshold:\n",
        "        gain_loss[word][sense] = \"-\"\n",
        "      else:\n",
        "        gain_loss[word][sense] = 0\n",
        "\n",
        "changed = {word: int(any(val for val in gain_loss[word].values())) for word in gain_loss}\n",
        "\n",
        "print(f\"Accuracy: {get_accuracy(changed, word_to_label)}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6896551724137931\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRc58ZxSAnqa",
        "colab_type": "code",
        "outputId": "338c886e-ec95-4d9c-a3bf-d58228565a07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# using plain counts instead of normalizing them\n",
        "\n",
        "changed_using_count = {}\n",
        "for word in word_sense_to_freq_corpus1:\n",
        "  if word_sense_to_freq_corpus1[word]:\n",
        "    for sense in word_sense_to_freq_corpus1[word]:\n",
        "      count1 = word_sense_to_freq_corpus1[word][sense]\n",
        "      count2 = word_sense_to_freq_corpus2[word][sense]\n",
        "\n",
        "      if (count1 <= 2 and count2 >= 5) or (count1 >= 5 and count2 <= 2):\n",
        "        changed_using_count[word] = 1\n",
        "        break\n",
        "    \n",
        "    if word not in changed_using_count:\n",
        "      changed_using_count[word] = 0\n",
        "\n",
        "changed_using_count\n",
        "print(f\"Accuracy: {get_accuracy(changed_using_count, word_to_label)}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5862068965517241\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}